{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import getpass\n",
    "import gmaps\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "from urllib.parse import urljoin\n",
    "from IPython.display import clear_output, display\n",
    "from parse import parse\n",
    "import yaml\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from shapely.wkb import loads\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "from shapely.geometry.point import Point\n",
    "from shapely.ops import cascaded_union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import psycopg2\n",
    "import json\n",
    "from fabric2 import Connection\n",
    "\n",
    "#adding prognos tools\n",
    "sys.path.insert(0, \"../../PROGNOS/\")\n",
    "from prognos_tools.encrypt import decryptString\n",
    "from prognos_tools.Basin_fabric2 import Basin\n",
    "\n",
    "\n",
    "storageToken=b'gAAAAABdiMKRN9CAWkwJjaORtRqmHmoAqR8j02XBp5PA18aPsIGdyMDAXN-pTYBjNAr0AaoZ6-kO2kS5x4fbFJFOZhk298W518PEhgQNIkot-dkCJfgk369_AEHoOtqUd84hnrpO5v1a58mu9VGA74KwEpEk3mDEdkZVPffPgK1y5B8Gp_s9_iqvPKP8EC4M-i1CVIhRIOVGEK4udkY-BYP-B20tYHarrbAigGwxeFxfpnSBQKUOtu4poU0GuJT0CLY7VUhqFb-0qTWQBmWqRg1e98_uMedZbGPjNKx1RAg7Z43-yb3M3OSFcaniRtXvzS5ScmVbsRuEWU2bR2hn2HWZY6m5oZdQPBWCoAknelAu7F3nMH3o-hGUciVDVaRGwGiD2qauyoPcbP0oP9kvjwSsysyFsomiRpewpaAkr2O85i0mXvehvGMMDZAKVFrCwJbXHR598ccCOp28Jo2-N91vadryW_-isdXF4vxKrlPwNDTqvy3m9WoxoeS98X5Z8U1cfbjVpy-UAzB16IzkivdkeOAteM_J6lZq2H5pnbG1TeF_f2kd3ul1FKV9xZGVxaBvT0G-YxjHM6HpYQn9O8rC_DfJUunoFToZp3VvG6hcTKjOl2inbaqqhcQ4RhavCymFFkR1FeMQ_2scNJcpiGUkI3GPGV7ujPlSpXVMkEViitT89thIIwsFL0IWvctJE35j42HArPhcEvvYWKBypt2Ok1jnVTNx7Vp4dE7V5HuXvkg65tjeHu2EFBVaIomhTdnqa2dUZ0Rj4F09J8s7NWA965V__LIp-30pSJaLKwAoRVs3DrkF11aV5EURNZyryXi6YwrLPCV5TodKvCmbCS-t1EVDpd50MX4KJErtuTAESvOQZTdFPMF0lR7ivMmc39luDOtSHupg0IiOqVaUVpw7GEF0FX4o4RZ61D4dx2xQ6nDBm0nyxOphGzkbHIGZmj9AnxTqVMDxfrFz9Edr2FBL4i3tORtIdGS_iDbiJvXm3fl1oYqe3bNqt_U-PKzlP9AXpsEkxyDPr6cNBqOE5coSFh9w1rJXfcdyQPt1p-57JydFQJmx2KvRmeYzXuruI1etqDF9eUzVVzC2GZfA__39-JFZMUDtNGsAwywbV6VB-xaNSi85wy7rjIitnWvo5tjqo1H8Hr-OFhrgvfKiXpWEF1_lBNIWqAnmPME8s-5CXIphMqKndp2ShMERviQX8C5aUosx3aIbnUboaUjxlIjiwCb1oXM_1_SRdoMmhKY87DibiRBJF_z1ouKmPnlUg-SAKYqh307Wd839PhlnxN8srVv8TdP7TkGlHZ_SoX6luloHB33Ejjuak0b0jG6o4yfyYgXHDDVkNRE7myeMhH4PKNqbLnG1USHxxsbT63h5R3LDwpU1MzjaGfEL3Mf86mTH6gsapWbImhtqlE1Orjq8Igx0O7tILowUBjmdBXcQw5Q4rhwe2ik-e18HnLycqCbq1ATcf2ldBmjpawzXUNVvZgqMq57eTraKgdM338gRT62Y2rVBZeFTURp-IIdN5NoWxBGnzFwyYPdq9aVXKtue9cDgeTHh3oP6lI9edu-baeGHi0bMzqPZppuht0-gZ1vVnEjapoMfj6hiqlfWUyG7Y4RqjQGabzAYxam0BhDXpSfvvoQCat02IyUG93d4gQQEjsyPQuVxV5pgKNA9YO99TqnKac2Xkr-zlqQyFe4J2NzkqDb-XSQXjOv2zkXaUZdFp54KnAb1iuc_tOgHZwpeMb_AkG9K_ACX2x7arG5slhNlUu-UXfLL5K9GdGXGQYJllyW3HTfi3ZjJthHn16fjMoxmzJU-qzqoKTT8HuU3sY9LLzKQJmlhpE915RDOsSlI6-znbTrqfGz_bm2putOb38HrH5i7ptR-ofCtblFa_jpYTx2R_A-2LSIokfOknXY7WPGciDY7YUzC_mz0SJYtNfQxF4ny6hKHVVh2Rt2XeP6-NsIqUaYrEDiFTz-kvXMA9MUQ70_z67oKr8dp8PK-kSPE_wdnjS1cUFVB1hhTm8HNsKqDNNB5byk7QN0GuTYXNk0hxKJK3i32UMclD8lLAP8Pa_nY4J47q84f3XSkNxJ571Lnr7RXSV88u3T6qnXWv0ciG4PAxXVwD_X2qpEY8-SyQqny2Yxw7xPbdwa7h6DrzXlzrk2pvI9KhEGuLtWMxiPOxiitDg0FEFJSHbfXZHKtqtFemav19nNrcBN-VbGLCt_bBVV4uuMs3W7nv-Y7MKFpKzQVdqYHbgbtSE3rgXQyLL_OirmxaBJz-DQXdystdR4epjtd1LNN8VXwnRv5Ln6siPPCy-QPp-ITKWe-ACsS3t3gIxTjgMpTVBbYp40OHfMwhk2iHDTA-DFlrilepwH8TWZG4LVl5fwHmv_rTpb_K01kv2_wAHZjCsNCtfTMiNjZTYepp3w_MbTG1KnV21L7UCi8ktj55W4vu4sh8REqiGfYDFhPJ444dae_-Wg_UdBwVrJtRWlPUW_m8V4kOgusQSgHPZCXR_SLAxWvbxd1MkzcXbIaCTj2hsseQFx3kUmNDj911E7rV1RbLRzaIFx6Weffy0U8mvrBh8kfZuHk8hAksdD-Ke4JC25ucn36y8MjGkV_bWf-h6I-2eijxg7da-3-R6EtPp8goTIkiFTO8IK84PEFnODhBvdGTYR81zMBmaYWnD3Ir0LZE4dGVc9jFYk22fZJ56httDKR130AsZ1B-nqlsfn-n9plJKoKWVjYLnfAI2TgOCndQCJ2fxkbjfS0x7eKCysUEijLX8vAqhwHCZOasikFlMGxmlHF-Ph_WunExoWhoxlvzP6FDny-0poTkllSRt9KN6rHn5rzqzxS-fSxW6GLnSn8Y7TsGpy0tn_KMRsGV5xbDJWFFMnvMf7hB1L9_Ut9fvzsX3Iz-O1ktqWnNQwBKi-MsFePpIXtKA4U8KQrWBf6TyASoqw79xVdlqbXwzYfg9Ur_kYh7nHXYKwP4oy2agTdYRzUydWi819NmgW_QFp72aUMGAKCxy5MpxJMDR_41OkQxyjuPHT_3HskCFjdcvUOgp0Hi8m-iU5D34PaVeGisC2H-rbyaS_ET3PtJ0kU1Xx3oF2xYM1q-4QB9qBxUyDdk5hBNxaoAslA8sE_Y1JfMzfyjIqK4OR5_noBkksj_f7tlezivS8eKmhClnSGboQVS5ypcEw5n8d-XQh8C58qKmQ2jUZOYyt6WBAD-TfzyqPxDX423z-Cuw=='\n",
    "computeToken=b'gAAAAABdiMIg9OCnTJ3gsI_ZGqzRz2jbkLvVHtvw8stG7ljodd08gXOQFYuiclCtsPkSLgwvMiuro1QZw_8Y8txjIPBVOI313SGpfNj5UtfG1jZbNeMDMHJXz3w_be3a6JjsLJtXSmFNYrTKUJKioomQuugq0dI8oIgqQ5MrAx8hOheG1BB0lxW20UzE5W2izjXLqGhtBGQJVrvvt7j1-GFcDu-FvSEik4JQQK9miOJlfNfPUhY5wagCcC_FLmdc_9vmBVo7xprhGkQjFmO_EbQJVkNIteE0QWt77x2zPkuOowyMxfDAP9KyOte2q5vrrX27InZ4OXNdoAP56kFYJ0dvqiq7vkPK2cliroFo3fDgm-SyPxZx4S37ga8aOanpESKpKZke7ea-EQoAOQqGGk9OA8tVDa2kJNvZEoZLa0xPI1rBiMPna3PWHoinfO0C3ohuDYIUPYKWNrymNmrNvdtkUMD9COvAWGP-wEZ7S7iZN9NlBJ_uUmFVpTNGVwBrGCebxW3byN5CSxQ7zKHuvlWI0-sggUph58lTYq1FThwZ2YM0hPDmO4Wvlefneyf3OYAURfAjU4Z9v8SZc2EUgb8jHLscTwY1hQWdVFDEgxFJ2vLOXX3yvfPfzNw0Fz9OlU2hwLqyZNYE7XoZaC2A2ndl9OjsKalqeyAmUoyRx-EN1IAMt-cvMwCXCiCzKgu1tmObye_evqcW2KRZ2hgvUBXWX_gtqDLa9RumxnKw9mw8aTfqoCumK74Vr9c4L6csJOkJyoZCDeKyfvMRgIPnFR4GThTZjdBRE4jMFnPV5I_pjcfNSX8gEXoKuH7pcWSMmSUZ2-IPTZeJVBT4yFpamKuYhao67ZVOCeIg0RTwnywuQZK3VGNqL0c27z_L_Y7uW-T0MV-Z8wv0hxqOT5JQ_xl_OIVGCUitQRnrN_ZIECIr6lpdhYk8HH73yOGKGtR6lMFK7ww99z4fWFl5X0opYrbMpI5dJ8kxB7u85escismPL2FFvib4s69hzBPAdXVaos2j1aquixf3pYIOOtnQE1_H0-JHZnmtfg4SBIUvyFVeWpxgBfcgvS-3YiUJLf5BU9hiSYLMMzIJbQspKz_qh3rrcYwvYEyUffrqD2p-62D95YqL4SNtO0l9nnzCZ909b1sddEoF1W_qjxNFc92G3B0tqC8vgEp_3Ec1b3A8fPAmSHV1MEXfEhPLha_OUiKLA8tGCN9lA5kee4712SSO6C0ImImZt-UB2RnzKveqtUI6Id1qM4Evs_lDKiaIWl8kDL-dXROiTWsLbHY454n0RRmUYLnB2UXlVwUZwN8r12rbbTlQ8u6G-Zwujnb8Nex9PpdCQ2gV72-860TNWxPQHNUvOCbXKK3ZnolEPd_mtx0gXNO3N99nNc2G4h9Ro1zWHP8w2UWnBEYPLluqqCNc71w5Xw4rQeNlXSy9-UixdZzV-5JGXggj0cMMgC9dpdj_lagShtms5REijpEOMguhuSTLmYNNNXsPD7Lp7yz7qFCvdtQuIHYoqZXvuHEH3mbBr56C2eQ-m_VPVnO0zkB-1aWgRDm20g0xsxUH2oheVo85hSw_H7-lZNhMoBvn6M_owIrfo9ntm7k4DGWHqVdzHXL30S2QarUvR1r7oq4E4SJKhgsZp4OphXJs6mzSNCUWEabcsKwx9wSXJex1PuWypIASji5sweL97kSWxz-7K26DUx73C7jFzDhKk4zwTm5bj08qc1juN7KJl6xKMdeRMb-qI7jeL_W8lqqscKRXCmzGsDuYwpq5zP3z76C2-YCVMZS5fuI299AwPkzhG5aYc-2yaRaHW-dIDnwdzHdGnC5EL1MH6SUdeCw1vhdbD42gEKOc-FJGZdsFdbVKN4iHKiP7Ewp5RcxOmZEnXOVFDz2mDj_oXJXuuxj7EpGhSltGo58CpqDp9PXMqSqZ25mRB2ZUNHyKQxDtiItdnZzfXRbe0l_2oWrDdygvPbaUiV7pNf9V_owZpAKw0pEN_mQ0TbKbPDs-0RkImfLHUN8en26XDtUSVMlyH_coyAzTYpxpzhOclcTCq-qt9BmWPUd95ns4Qst5JLHXE5eA4aaFqAgKxyFtDIBb3G_cfCF4yjYxzmC3g7HgU0mQXAzzJ3-f8GYJeaVNLzKSnhx_lEDgvzhokc3dAiL6wyxqPgkYq6VvjqGqA1s85673gbKXB2wtfCKjaU2HcYZxPooX9JSWLUJ7dmwJm7CPUzLUPNu3jrk0LUJVe9MAugiKAgOj823uT-TiS4vu3M25uEs2Z8wJ38G-_8T24AYQkAUQuBTsVMssDIW8p_Yz2SWPwaJN9rzNIA_CI0JI0k60kWOmdNEPqjdcxAZgmCUBjanjhc8gxlMtVxbOSqewi95hNuuyd7gXKiRKvb4sMpqXviKe3HKFqdRvmfFBaWYKymLOx0-bxhwZkwjpZeKlh3oK4QCXbMedJ1pPOHY4nCfh4_TSbIKQPlC68ItsmsRENaiffwXz0mEQbYVCkWZ_gYnSER9Lc5P4-GBE3nQtxLEHOQQra0d5g0TLMDx0WjRv1M8hGZ8zUBHILuab_Y0jOURKYtJJLNIyHhH0qdFktk_0CoJplzL1Vu03ZUYGIS7zoEAFio9-MTGZ-zBuPow7flAS0V3VnSxRDlgY8Fn_Uc044EMlQ0Oszz8vIpUtAf9oAWpvu-wmtYrU2I2mrLUyxf2doG0O5lMwhEC7WKBaI4YYzwSt5NTaAZFVy-pe6hq4f0NBlvB083Gi5-eT_s9qOsagf2Rd9ssV9aZY4QNuqobTZTIOFqytZrraRfGAClGyppZlLrIlkOexIJBGEcwObYLhdPipdazozE4FRLjhDrAFqutE8yHeR_irOJ-20x8AYxp3q4xjR0D5xeyVy6RgWm8hhhEoPEX9XZkFrwho1J_TOuEAWCDy6D2pdK1DEcJmEE8MEUVKGJn4q6Rtn8uHSN8iz5dzwqifQFlgRDQahWUwOJhvQ1PgQSegeR2-bjRkWix2ap8CMFXcJAOwE01WggE8PSYhP9JKrSUsJPEJVOPwImWnvUhoQfsi6u4TeMqAvwOy87Z1NKIBleHAky1YwICJSnMmRPGHoB-Xfq32p3nG5XlHCsB1jr7FRifGqKoyA22hSHspqA7ZpwwitDLw3lzJ0HfZsuG91e2rAx2I5h0khw=='\n",
    "gmapsToken=b'gAAAAABctxJp5wE73qK6U5VieCi0WXNnNex4KxLZHutsESa8fW9v43lLa1Ag0qsxKFjXXo9MBAdvUpqJPW-QmCE0gH_Opf9g4xAG1VaI2WarO_xDZg44VLMCHkd_6O8ofgp8u4VuFBMr'\n",
    "key = getpass.getpass('Password: ')\n",
    "apiKey = decryptString(gmapsToken,key)\n",
    "computeKey =  json.loads(decryptString(computeToken,key))\n",
    "storageKey =  json.loads(decryptString(storageToken,key))\n",
    "with open('compute.json','w') as f:\n",
    "    json.dump(computeKey,f)\n",
    "with open('storage.json','w') as f:\n",
    "    json.dump(storageKey,f)\n",
    "gmaps.configure(api_key=apiKey)\n",
    "del key,apiKey,computeKey,storageKey\n",
    "\n",
    "#Creating folder to store temporary files and downloaded data\n",
    "saveFolder = './DownloadedData/'\n",
    "\n",
    "args = [saveFolder,]\n",
    "!!rm -rf {args[0]} && mkdir -p {args[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting list of openDAP links with high-resolution (1km2) temperature and precipitation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getList = False\n",
    "\n",
    "baseURL = 'http://thredds.met.no/thredds/catalog/ngcd/version_19.09/catalog.html'\n",
    "# varsToGet = ['TX','TN','TG','RR']\n",
    "varsToGet = ['RR','TG']\n",
    "model = 'type2'\n",
    "listFolder = os.path.abspath('./' + model + 'Lists') #Folder where list of links will be saved\n",
    "\n",
    "args=[]\n",
    "if getList:\n",
    "    with Connection('localhost') as c:\n",
    "        c.local('rm -rf {}'.format(listFolder))\n",
    "        c.local('mkdir {}'.format(listFolder))\n",
    "    \n",
    "\n",
    "only_a_tags = SoupStrainer(\"a\", href=True)\n",
    "\n",
    "def getSoup(url,re_str):\n",
    "    request=requests.get(url)\n",
    "    soup=BeautifulSoup(request.content,'lxml',parse_only=only_a_tags)\n",
    "    link_soup=soup.find_all('a',text=re.compile(re_str))\n",
    "    links=[]\n",
    "    for i in link_soup:\n",
    "        links.append(urljoin(url,i['href']))\n",
    "    return links\n",
    "\n",
    "def getFileList(var):\n",
    "    allLinks=[]\n",
    "    for var_link in getSoup(baseURL,var):\n",
    "        for model_link in getSoup(var_link,model):\n",
    "            for year_link in getSoup(model_link,'^[0-9]{4}/$'):\n",
    "                print('Processing {}'.format(year_link))\n",
    "                for month_link in getSoup(year_link,'^[0-9]{2}/$'):\n",
    "                    for day_link in getSoup(month_link,'\\\\.nc$'):\n",
    "                        for opendap_link in getSoup(day_link,'^/thredds/dodsC/'):\n",
    "                            allLinks.append(re.sub(r'\\.html$', '', opendap_link))\n",
    "    return allLinks\n",
    "    \n",
    "# display(listFolder)\n",
    "if getList:\n",
    "    links = []\n",
    "    for i in varsToGet:\n",
    "        #Getting all links for variable\n",
    "        links.append(getFileList(i))\n",
    "        #Saving paths to file\n",
    "        with open(os.path.join(listFolder , i + '_' + model + '.txt'), 'w') as f:\n",
    "            for item in links[-1]:\n",
    "                f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying extent of the basin\n",
    "### Creating a VM with the geodatabase on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing connection to google cloud\n",
    "cloudInfo = {'project': 'nivacatchment',\n",
    "             'zone': 'europe-west3-a',\n",
    "             'instanceType' : \"n1-standard-2\",\n",
    "             'instanceName' : \"basin\",\n",
    "             'username' : \"jose-luis\",\n",
    "             'keyDir' : './'\n",
    "             }\n",
    "basin=Basin('compute.json',cloudInfo,'storage.json')\n",
    "display(basin.properties)\n",
    "\n",
    "info = basin.get('projectInfo')\n",
    "display('Can now talk to project {}'.format(info['name']))\n",
    "\n",
    "\n",
    "#Setting ssh firewall for project\n",
    "inOffice = False\n",
    "if not inOffice:    \n",
    "    with Connection('localhost') as c:\n",
    "        hubIp = c.local('curl https://ipinfo.io/ip').stdout.strip()\n",
    "        basin.setSSHPort(hubIp,inOffice=False)\n",
    "        basin.setPostgresAccess(hubIp,inOffice=False)\n",
    "else:\n",
    "    basin.setSSHPort()\n",
    "    basin.setPosgresAccess()\n",
    "\n",
    "#Actually instantiating the machine\n",
    "ip=basin.instantiate(wait=True) #wait until the vm is actually up and running\n",
    "display(ip)\n",
    "\n",
    "#Allowing local connections on the VM without password by modifying the pg_hba.conf file\n",
    "with Connection(host=basin.properties['ip'],\n",
    "                user=basin.properties['username'],\n",
    "                connect_kwargs={\"key_filename\": basin.properties['keyFile'],}\n",
    "                ) as c:\n",
    "    #c.sudo('''find /etc -name pg_hba.conf -exec sed -i 's|\\(^host \\+all \\+all \\+::1/128 \\+\\).*|\\\\1trust|g' {} \\; ''') #Trust all local connections\n",
    "    #c.sudo('''find /etc -name pg_hba.conf -exec sed -i 's|\\(^host \\+all \\+all \\+127.0.0.1/32 \\+\\).*|\\\\1trust|g' {} \\; ''') #Trust all local connections\n",
    "    c.sudo('''sed -i '1,/# IPv4 remote .*/!d' /etc/postgresql/9.6/main/pg_hba.conf''') #Deleting all settings for ipv4 connections\n",
    "    c.sudo('''echo \"host    all    all    151.157.0.0/16   md5\" | sudo tee -a /etc/postgresql/9.6/main/pg_hba.conf >/dev/null''') #Allowing connection from NIVA ip\n",
    "    c.sudo('''echo \"host    all    all    {}/32    trust\" | sudo tee -a /etc/postgresql/9.6/main/pg_hba.conf >/dev/null'''.format(hubIp)) #Allowing connection form hub ip\n",
    "    c.sudo(\"service postgresql restart\")\n",
    "\n",
    "\n",
    "#Defining custom queries to geodatabase. Note that we forwarded the port on the VM to the\n",
    "#local 5432 port so as to be able to talk to postgresql through the local port\n",
    "def query(query,fetch=True):\n",
    "    db = psycopg2.connect(host=ip, port=5432, database='geonorway')\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(query)\n",
    "    if fetch:\n",
    "        result = cursor.fetchall()\n",
    "        db.close()\n",
    "        return result\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting basins extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin.setConnection()\n",
    "schema = 'metno'\n",
    "station =  'Vansjø'\n",
    "stationsFile = '{}.yaml'.format(station)\n",
    "stations = yaml.safe_load(open(stationsFile))\n",
    "display(station)\n",
    "#Actually processing the basin\n",
    "geoResultsFolder='./geoResults'\n",
    "basin.getBasinLayers(stationsFile,schema,saveFolder=geoResultsFolder)\n",
    "\n",
    "\n",
    "sql = '''UPDATE metno.resultsshp AS a\n",
    "SET basin = (SELECT ST_Union(b.basin)\n",
    "FROM metno.resultsshp AS b\n",
    "WHERE b.station_id = 0 OR b.station_id = 1)\n",
    "WHERE a.station_id = 0;\n",
    "DELETE FROM metno.resultsshp AS a WHERE a.station_id = 1;\n",
    "ALTER TABLE metno.resultsshp\n",
    "ADD COLUMN area DOUBLE PRECISION;\n",
    "UPDATE metno.resultsshp\n",
    "SET area = (ST_Area(basin));\n",
    "'''\n",
    "\n",
    "with open('sql.sql','w') as bla:\n",
    "    bla.write(sql)\n",
    "\n",
    "basin.connection.put('sql.sql')\n",
    "basin.connection.run('psql -d geonorway -f sql.sql')\n",
    "basin.connection.run('rm sql.sql')\n",
    "# basinGenerator.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding user jovyan \n",
    "sql = '''CREATE ROLE jovyan WITH LOGIN;\n",
    "GRANT USAGE ON SCHEMA metno TO jovyan;\n",
    "GRANT SELECT ON ALL TABLES IN SCHEMA metno TO jovyan;\n",
    "'''\n",
    "\n",
    "with open('sql.sql','w') as bla:\n",
    "    bla.write(sql)\n",
    "\n",
    "basin.connection.put('sql.sql')\n",
    "basin.connection.run('psql -d geonorway -f sql.sql')\n",
    "basin.connection.run('rm sql.sql')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = query('''SELECT json_build_object('type', 'FeatureCollection',\n",
    "                                      'features', json_agg(json_build_object(\n",
    "                                                                'type',       'Feature',\n",
    "                                                                'label',      station_name,\n",
    "                                                                'geometry',   ST_AsGeoJSON(ST_ForceRHR(St_Transform(basin,4326)))::json,\n",
    "                                                                'properties', jsonb_set(row_to_json(resultsShp)::jsonb,'{basin}','0',false)\n",
    "                                                                )\n",
    "                                                            )\n",
    "                                     )\n",
    "             FROM metno.resultsShp;''')\n",
    "\n",
    "fig = gmaps.figure(map_type=\"TERRAIN\")\n",
    "fig.add_layer(gmaps.geojson_layer(a[0][0]))\n",
    "\n",
    "b = query('''SELECT a.station_name, st_x(st_transform(a.outlet,4326)),\n",
    "    st_y(st_transform(a.outlet,4326)), st_area(b.basin)/1e6\n",
    "    FROM metno.demShp AS a\n",
    "    INNER JOIN metno.resultsShp AS b \n",
    "    ON a.station_id = b.station_id''')\n",
    "\n",
    "outlets = [{\"name\": i[0], \"area\": i[3]} for i in b]\n",
    "locations = [(float(i[2]),float(i[1])) for i in b]\n",
    "info_box_template = \"\"\"\n",
    "<dl>\n",
    "<font color=\"black\">\n",
    "<dt>Name</dt><dd>{name}</dd>\n",
    "<dt>Area</dt><dd>{area} km<sup>2</sup></dd>\n",
    "</font>\n",
    "</dl>\n",
    "\"\"\"                                                \n",
    "outlet_info = [info_box_template.format(**outlet) for outlet in outlets]                                                 \n",
    "marker_layer = gmaps.marker_layer(locations, info_box_content=outlet_info)\n",
    "fig.add_layer(marker_layer)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data from metno thredds servers\n",
    "## Creating a virtual machine with the fimex utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing connection to google cloud\n",
    "hasBeenDownloaded = True\n",
    "if not hasBeenDownloaded:\n",
    "    cloudInfo = {'project': 'nivacatchment',\n",
    "                 'zone': 'europe-west3-a',\n",
    "                 'instanceType' : \"n1-standard-16\",\n",
    "                 'instanceName' : \"fimex\",\n",
    "                 'username' : \"jose-luis\",\n",
    "                 'keyDir' : './',\n",
    "                 'image' : \"projects/nivacatchment/global/images/fimex\"\n",
    "                 }\n",
    "\n",
    "    fimex = Basin('compute.json',cloudInfo,'storage.json')\n",
    "    fimex.instantiate(wait=True)\n",
    "    fimex.setConnection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating cfg file for download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgTemplate='''[extract]\n",
    "reduceToBoundingBox.south={south}\n",
    "reduceToBoundingBox.north={north}\n",
    "reduceToBoundingBox.west={west}\n",
    "reduceToBoundingBox.east={east}\n",
    "selectVariables={variable}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all variables for all basins using fimex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting all boundaries to download:\n",
    "b = query('''SELECT station_name,Box2D(ST_Transform(St_Buffer(St_Envelope(basin),2000),4326)) FROM metno.resultsShp;''')\n",
    "names = [i[0] for i in b]\n",
    "boxes = [parse('BOX({west} {south},{east} {north})',i[1]) for i in b]\n",
    "display(names,boxes)\n",
    "\n",
    "#parallel fimex script\n",
    "geoResultsFolder='./geoResults'\n",
    "script = '''#! /bin/bash\n",
    "n=$((`nproc` * 2))\n",
    "rm -rf ./tmp\n",
    "mkdir ./tmp\n",
    "ulimit -n 1024\n",
    "cat {fileList} | parallel --eta --jobs $n -u --no-notice \"fimex -c {cfgFile} --input.file={{}} --output.file=./tmp/{{/}} > /dev/null 2>&1\" \n",
    "ncrcat ./tmp/*.nc {out}\n",
    "rm -rf ./tmp\n",
    "'''\n",
    "#> /dev/null 2>&1\n",
    "\n",
    "if not hasBeenDownloaded:\n",
    "    args=[]\n",
    "    for i in varsToGet:\n",
    "        #Getting only dates from 2010 to 2016\n",
    "        args = [os.path.join(listFolder, \n",
    "                              i + '_' + model\n",
    "                             ), \n",
    "                 \"1970\", \"2019\"\n",
    "                ]\n",
    "        !cat {args[0]}.txt |  grep -E '/'`seq -s /\\|/ {args[1]} {args[2]}`'/' > {args[0]}_mini.txt\n",
    "       # fimexGenerator.callPopen('head -n 40 {0}.txt > {0}_mini.txt'.format(i + '_' + model))\n",
    "        currentDict = boxes[0].named\n",
    "        currentDict['variable'] = i\n",
    "        display(currentDict)\n",
    "        with open('box.cfg','w') as f:\n",
    "            f.write(cfgTemplate.format(**currentDict))\n",
    "        for j in names:\n",
    "            filename = i + '_' +  model + '_mini.txt'\n",
    "            display(\"Downloading from all files in  {}\".format(filename))\n",
    "            with open('script.sh', 'w') as f:\n",
    "                out = j+'_'+i + '.nc' \n",
    "                f.write(script.format(fileList=filename,\n",
    "                                      cfgFile='box.cfg',\n",
    "                                      out=out)\n",
    "                        )\n",
    "            fimex.connection.put( os.path.join(listFolder, filename) )\n",
    "            fimex.connection.put('box.cfg')\n",
    "            fimex.connection.put('./script.sh')\n",
    "            fimex.connection.run('chmod +x {}'.format('./script.sh'))\n",
    "            fimex.connection.run('./script.sh')\n",
    "            fimex.connection.get(out,'./'+out)\n",
    "            fimex.connection.run('rm -rf box.cfg script.sh {} {} ./tmp'.format(filename,out) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersecting basin and nc file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a grid from the nc file\n",
    "\n",
    "temperatureFile = os.path.abspath('{}_TG.nc'.format(station)) #'./'/home/jose-luis/Downloads/hist_CNRM_CCLM_RR_daily_mm_1971.nc\n",
    "nc_data = xr.open_dataset(temperatureFile)\n",
    "#temperature = nc_data.precipitation__map_hist_daily\n",
    "temperature = nc_data.TG\n",
    "nc_shape = temperature.shape\n",
    "#X = nc_data.Xc.values\n",
    "# Y = nc_data.Yc.values,\n",
    "X = nc_data.X.values\n",
    "Y = nc_data.Y.values\n",
    "# lat = nc_data.lat.values\n",
    "nc_data.close()\n",
    "\n",
    "#Y=Y[0]\n",
    "\n",
    "cnt = 0\n",
    "grid = list()\n",
    "for i in X:\n",
    "    for j in Y:\n",
    "        left = i-500\n",
    "        right = i+500\n",
    "        bottom = j-500\n",
    "        top = j+500\n",
    "        p1 = Point(left,bottom)\n",
    "        p2 = Point(left,top)\n",
    "        p3 = Point(right,top)\n",
    "        p4 = Point(right,bottom)\n",
    "        pointList = [p1,p2,p3,p4,p1]\n",
    "        poly = Polygon([[p.x,p.y] for p in pointList])\n",
    "        poly.sid = cnt \n",
    "        grid.append(poly)\n",
    "        cnt += 1\n",
    "\n",
    "grid = MultiPolygon(grid)  \n",
    "display(grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = False#True\n",
    "if transform:\n",
    "    import pyproj\n",
    "    from shapely.ops import transform\n",
    "\n",
    "    project = pyproj.Transformer.from_proj(\n",
    "        pyproj.Proj(init='epsg:32633'), # source coordinate system\n",
    "        pyproj.Proj(init='epsg:3035')) # destination coordinate system\n",
    "\n",
    "    grid = transform(project.transform, grid) \n",
    "    display(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading the grid to the geodatabase\n",
    "sql = '''UPDATE metno.resultsshp SET basin = ST_MakeValid(basin); \n",
    "DROP TABLE IF EXISTS metno.dataBoundaries;\n",
    "CREATE TABLE metno.dataBoundaries(sid SERIAL PRIMARY KEY, boundary geometry(MULTIPOLYGON, 3035) );\n",
    "INSERT INTO metno.dataBoundaries(boundary) VALUES (ST_SetSRID('{hexStr}'::geometry,3035));\n",
    "DROP INDEX IF EXISTS metno_data_boundary_gix;\n",
    "CREATE INDEX metno_data_boundary_gix ON metno.dataBoundaries USING GIST(boundary);\n",
    "DROP INDEX IF EXISTS metno_basins_gix;\n",
    "CREATE INDEX metno_basins_gix ON metno.resultsShp USING GIST(basin);\n",
    "'''.format(hexStr=grid.wkb_hex)\n",
    "\n",
    "with open('sql.sql','w') as bla:\n",
    "    bla.write(sql)\n",
    "\n",
    "basin.connection.put('sql.sql')\n",
    "basin.connection.run('psql -d geonorway -f sql.sql')\n",
    "basin.connection.run('rm sql.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersecting grid with basin and getting area of the intersection\n",
    "sql = '''\n",
    "DROP TABLE IF EXISTS test_dump;\n",
    "CREATE TABLE test_dump AS\n",
    "SELECT (ST_Dump(boundary)).path[1] as sid, (ST_Dump(boundary)).geom as geom\n",
    "FROM metno.dataBoundaries;\n",
    "\n",
    "CREATE INDEX dump_idx ON test_dump USING GIST(geom);\n",
    "\n",
    "DROP TABLE IF EXISTS subdivided_geoms;\n",
    "CREATE TABLE subdivided_geoms AS\n",
    "SELECT ST_Subdivide(basin,32) AS geom\n",
    "FROM metno.resultsShp\n",
    "WHERE station_name ='{}';\n",
    "\n",
    "CREATE INDEX subdivided_idx ON test_dump USING GIST(geom);\n",
    "\n",
    "DROP TABLE IF EXISTS metno.areas;\n",
    "CREATE TABLE metno.areas AS\n",
    "WITH biglim AS (\n",
    "    SELECT a.sid,ST_Area(ST_Intersection(b.geom, a.geom)) AS area FROM  test_dump AS a, subdivided_geoms AS b\n",
    "    WHERE ST_Intersects(a.geom,b.geom)\n",
    ") \n",
    "SELECT sid,SUM(area) as area FROM biglim\n",
    "GROUP BY sid;\n",
    "\n",
    "GRANT SELECT ON ALL TABLES IN SCHEMA metno TO jovyan;\n",
    "'''.format(station)\n",
    "\n",
    "with open('sql.sql','w') as bla:\n",
    "    bla.write(sql)\n",
    "\n",
    "basin.connection.put('sql.sql')\n",
    "basin.connection.run('psql -d geonorway -f sql.sql')\n",
    "basin.connection.run('rm sql.sql')\n",
    "\n",
    "areas = query('SELECT * from metno.areas;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting weight array from area file\n",
    "#weights = pd.read_csv( os.path.join(geoResultsFolder,'areas.txt'),delimiter=';', header=None, names=['idx','area'])\n",
    "\n",
    "idx = [i[0] for i in areas] #weights.idx.values\n",
    "weight = [i[1] for i in areas] #weights.area.values\n",
    "#Getting the in a 2d array\n",
    "weightArray = np.zeros((nc_shape[1:]))\n",
    "arrayIdx = np.unravel_index(idx,nc_shape[2:0:-1])\n",
    "for i,j,k in zip(arrayIdx[1],arrayIdx[0],weight):\n",
    "    weightArray[i][j] = k;\n",
    "\n",
    "plt.imshow(weightArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('{}_weights.txt'.format(station),weightArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Getting area-weighted data\n",
    "def getWeightedData(filename,variable,save=True,isTemperature=False):\n",
    "    nc_data = xr.open_dataset(filename,chunks={'time':1000})\n",
    "    time = nc_data.time.values + np.timedelta64(1,'D')\n",
    "    total_weight = sum(weight)\n",
    "    data = np.sum(np.sum((nc_data.variables[variable] * weightArray)/total_weight,axis=1),axis=1)\n",
    "    if isTemperature:\n",
    "        data-=272.15\n",
    "    nc_data.close()\n",
    "    data_df = pd.DataFrame({variable:data},index=time)\n",
    "    if save:\n",
    "        data_df.to_pickle('./{}.pickle'.format(variable))\n",
    "        data_df.to_csv('./{}.csv'.format(variable))\n",
    "    return data_df\n",
    "\n",
    "temperature_df = getWeightedData(os.path.abspath('./{}_TG.nc'.format(station)),'TG',isTemperature=True)\n",
    "temperature_df.plot(figsize=(20,5))\n",
    "fig=plt.gcf()\n",
    "fig.savefig('temperature.png')\n",
    "\n",
    "rain_df = getWeightedData(os.path.abspath('./{}_RR.nc'.format(station)),'RR')\n",
    "rain_df.plot(figsize=(20,5))\n",
    "fig=plt.gcf()\n",
    "fig.savefig('rain.png')\n",
    "\n",
    "rain_df.to_csv(\"{}_rain.csv\".format(station))\n",
    "temperature_df.to_csv(\"{}_temperature.csv\".format(station))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(rain_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data over lake\n",
    "### Putting lake in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For now this doesn't work on the server but we did it locally and uploaded the vansjo shapefile\n",
    "#with Connection('localhost') as c:\n",
    "#    c.local('''ogr2ogr -sql \"SELECT * FROM Innsjo_Innsjo WHERE vatnLnr='291'\" vansjø.shp /home/jovyan/watexr/Norwegian_lakes/Innsjo_Innsjo.shp''',replace_env=False)\n",
    "    \n",
    "basin.connection.put('/home/jovyan/watexr/Norwegian_lakes/vansjø.tar')\n",
    "basin.connection.run('tar -xf vansjø.tar')\n",
    "basin.connection.run('rm vansjø.tar')\n",
    "basin.connection.run('echo \"drop table if exists metno.lake;\" | psql -d geonorway')\n",
    "basin.connection.run('shp2pgsql -s 32633:3035 vansjø metno.lake | psql -d geonorway')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersecting lake with gridded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersecting grid with basin and getting area of the intersection\n",
    "sql = '''\n",
    "DROP TABLE IF EXISTS test_dump;\n",
    "CREATE TABLE test_dump AS\n",
    "SELECT (ST_Dump(boundary)).path[1] as sid, (ST_Dump(boundary)).geom as geom\n",
    "FROM metno.dataBoundaries;\n",
    "\n",
    "CREATE INDEX dump_idx ON test_dump USING GIST(geom);\n",
    "\n",
    "DROP TABLE IF EXISTS subdivided_geoms;\n",
    "CREATE TABLE subdivided_geoms AS\n",
    "SELECT ST_Subdivide(ST_Force2D(geom),32) AS geom\n",
    "FROM metno.lake\n",
    "WHERE gid = 1;\n",
    "\n",
    "CREATE INDEX subdivided_idx ON test_dump USING GIST(geom);\n",
    "\n",
    "DROP TABLE IF EXISTS metno.areas;\n",
    "CREATE TABLE metno.areas AS\n",
    "WITH biglim AS (\n",
    "    SELECT a.sid,ST_Area(ST_Intersection(b.geom, a.geom)) AS area FROM  test_dump AS a, subdivided_geoms AS b\n",
    "    WHERE ST_Intersects(a.geom,b.geom)\n",
    ") \n",
    "SELECT sid,SUM(area) as area FROM biglim\n",
    "GROUP BY sid;\n",
    "\n",
    "GRANT SELECT ON ALL TABLES IN SCHEMA metno TO jovyan;\n",
    "'''.format(station)\n",
    "\n",
    "with open('sql.sql','w') as bla:\n",
    "    bla.write(sql)\n",
    "\n",
    "basin.connection.put('sql.sql')\n",
    "basin.connection.run('psql -d geonorway -f sql.sql')\n",
    "basin.connection.run('rm sql.sql')\n",
    "\n",
    "areas = query('SELECT * from metno.areas;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [i[0] for i in areas] #weights.idx.values\n",
    "weight = [i[1] for i in areas] #weights.area.values\n",
    "#Getting the in a 2d array\n",
    "weightArray = np.zeros((nc_shape[1:]))\n",
    "arrayIdx = np.unravel_index(idx,nc_shape[2:0:-1])\n",
    "for i,j,k in zip(arrayIdx[1],arrayIdx[0],weight):\n",
    "    weightArray[i][j] = k;\n",
    "\n",
    "plt.imshow(weightArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting area-weighted data\n",
    "# def getWeightedData(filename,variable,save=True,isTemperature=False):\n",
    "#     nc_data = xr.open_dataset(filename,chunks={'time':1000})\n",
    "#     time = nc_data.time.values\n",
    "#     total_weight = sum(weight)\n",
    "#     data = np.sum(np.sum((nc_data.variables[variable] * weightArray)/total_weight,axis=1),axis=1)\n",
    "#     if isTemperature:\n",
    "#         data-=272.15\n",
    "#     nc_data.close()\n",
    "#     if save:\n",
    "#         data_df = pd.DataFrame({variable:data},index=time)\n",
    "#         data_df.to_pickle('./{}.pickle'.format(variable))\n",
    "#         data_df.to_csv('./{}.csv'.format(variable))\n",
    "#     return data_df\n",
    "\n",
    "temperature_lake_df = getWeightedData(os.path.abspath('./{}_TG.nc'.format(station)),'TG',isTemperature=True)\n",
    "temperature_lake_df.plot(figsize=(20,5))\n",
    "fig=plt.gcf()\n",
    "fig.savefig('temperature_lake.png')\n",
    "\n",
    "rain_lake_df = getWeightedData(os.path.abspath('./{}_RR.nc'.format(station)),'RR')\n",
    "rain_lake_df.plot(figsize=(20,5))\n",
    "fig=plt.gcf()\n",
    "fig.savefig('rain_lake.png')\n",
    "\n",
    "rain_lake_df.to_csv(\"{}_rain_lake.csv\".format(station))\n",
    "temperature_lake_df.to_csv(\"{}_temperature_lake.csv\".format(station))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basin.delete('instanceInfo')\n",
    "fimex.delete('instanceInfo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing lake and basin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_lake_df.columns = {'RR_lake'}\n",
    "temperature_lake_df.columns = {'TG_lake'}\n",
    "\n",
    "rain = pd.concat([rain_df, rain_lake_df])\n",
    "rain.resample('m').sum().plot(figsize=(20,5),marker='+',linestyle='None')#,linestyle='None'\n",
    "\n",
    "fig=plt.gcf()\n",
    "fig.savefig('rain_lake_basin.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = pd.concat([temperature_df, temperature_lake_df])\n",
    "temperature.resample('Y').mean().plot(figsize=(20,5),marker='+')#,linestyle='None'\n",
    "\n",
    "fig=plt.gcf()\n",
    "fig.savefig('temperature_lake_basin.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(rain.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
