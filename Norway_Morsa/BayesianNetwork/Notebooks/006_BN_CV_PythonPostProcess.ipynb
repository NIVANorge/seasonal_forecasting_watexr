{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, os\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import bayes_net_utils as bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "Notebook takes observed and predicted lake chemistry/ecol values output by notebook 02_BN_development_1Season (from the continuous network cross validation section). These are then discretized into WFD-related boundaries, and classification error is calculated. This classification error can then be compared to the classification error obtained by cross validation of discrete networks using the same classifications, or just to provide supporting error info to accompany predictions of probabilities of being in a certain class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT\n",
    "\n",
    "# Folder containing files of observed and predicted values from each cross validation run. Produced in notebook BN_development_1Season_R\n",
    "CV_obs_sim_folder = \"../Data/CrossValidation/LOOCV_predictions\"\n",
    "\n",
    "# Boundaries dictionary, copied from notebook B_seasonal_data_matrix_1Season\n",
    "bound_dict = {\n",
    "             'TP': [29.5], # No data below 20, so drop this class boundary. 29.5 is middle of 'Mod' class   \n",
    "             'chla': [20.0],  # WFD boundaries: [10.5, 20.0]. But only 6 d.p. under 10.5 so merge G and M classes and use 20.\n",
    "                              # For predicting cyano, would be better 17.4.   \n",
    "             'colour': [48.0], # 66th percentile\n",
    "             'cyano': [1.0], # M-P boundary is 2.0, but there were only 2 values in this class, rest above\n",
    "             }\n",
    "\n",
    "# Alter the boundaries in the boundaries dict for cyano, to take account of the box-cox transformation applied to the continuous\n",
    "# data:  y* = (y^L - 1)/L, where we used lambda = 0.1 when transforming original cyano data\n",
    "# bound_dict['cyano'] = [bound_dict['cyano'][0]**0.1 - 1]\n",
    "\n",
    "met_source = 'metno' #'metno' or 'era5'\n",
    "\n",
    "var_li = ['TP', 'chla', 'cyano', 'colour'] # What do you want to produce stats for? Need to have corresponding files in 'Data/CrossValidation/%s' %met_source folder\n",
    "\n",
    "# Pre-calculated standard deviations (from fitting of GBN in a later notebook)\n",
    "sd_fpath = \"../Data/FittedNetworkDiagnostics/GBN_%s_1981-2018_stdevs.csv\" %(met_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxcox_backtransform(x, lambda_param, bias_adj=True, sd_cyano=None):\n",
    "    \"\"\"\n",
    "    x: value to back transform\n",
    "    lambda_param: lambda used in box cox transform\n",
    "    bias_adj: bias adjust the values when back-transforming? True or False\n",
    "    sigma: if bias adjusting, this is the standard deviation of the box-cox transformed observations\n",
    "\n",
    "    returns: back-transformed value\n",
    "    \"\"\"\n",
    "\n",
    "    if bias_adj is True:\n",
    "        backtransformed_value = ((x * lambda_param + 1) ** (1 / lambda_param)) * (\n",
    "            1 + (((sd_cyano**2) * (1 - lambda_param)) / (2 * (lambda_param * x + 1) ** 2))\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        backtransformed_value = (x * 0.1 + 1) ** (1 / 0.1)\n",
    "\n",
    "    return backtransformed_value\n",
    "\n",
    "\n",
    "def xval_postprocess(var, fpath, sd_cyano=None):\n",
    "    \"\"\"\n",
    "    Function to read in a csv of observed and predicted values from a continuous\n",
    "    Bayesian belief network produced in BNLearn R notebook, and calculate correlation\n",
    "    coefficients, and then classify according to WFD and work out classification error.\n",
    "\n",
    "    Inputs:\n",
    "        var: string, one of 'TP','chla','cyano','colour'\n",
    "        fpath: string giving location of csv to be read in. csv should have columns:\n",
    "                'obs_1','pred_1','obs_2','pred_2',... where _1, _2, etc. is the cross\n",
    "                validation run number.\n",
    "        sd_cyano: standard deviation of the box-cox transformed cyanobacteria observations, for use when doing bias-adjusted back transformation\n",
    "\n",
    "    Returns:\n",
    "    Printed output, plus dictionary of results, with keys\n",
    "        'corr_coeffs': series of correlation coefficients, one value per cross validation run\n",
    "        'classification_errors': series of classification errors, one value per cross validation run\n",
    "        'cont_data_dict': dict of observed and predicted dfs, continuous data, one df per xval run\n",
    "        'classified_data_dict': dict of observed and predicted dfs, classified data, one df per xval run\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # Read in data\n",
    "    df = pd.read_csv(fpath, index_col=0)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # Split into separate dataframes for each cross validation run\n",
    "    cont_dict = {}  # Key: run number, returns df with obs and pred\n",
    "    for i, col_name in enumerate(df.columns):\n",
    "        if i % 2 == 0:  # If even, i.e. only do this for half the cols\n",
    "            run_no = int(col_name.split(\"_\", 1)[1])\n",
    "            if run_no == 1:\n",
    "                temp_df = df.iloc[:, [0, 1]]\n",
    "            else:\n",
    "                temp_df = df.iloc[:, [2 * run_no - 2, 2 * run_no - 1]]\n",
    "\n",
    "            temp_df.columns = [\"obs\", \"pred\"]\n",
    "\n",
    "            # If variable is cyanobacteria, transform observed and predicted to original data scale\n",
    "            if var == \"cyano\":\n",
    "\n",
    "                transformed_df = pd.DataFrame()\n",
    "                transformed_df[\"obs\"] = temp_df[\"obs\"].apply(\n",
    "                    boxcox_backtransform,\n",
    "                    lambda_param=0.1,\n",
    "                    bias_adj=False\n",
    "                )\n",
    "\n",
    "                # Calculate standard deviation of predictions on transformed scale\n",
    "                # sd_cyano = temp_df[\"obs\"].std()\n",
    "\n",
    "                # With bias-adjusted back transformation, to estimate the mean\n",
    "                transformed_df[\"pred\"] = temp_df[\"pred\"].apply(\n",
    "                    boxcox_backtransform,\n",
    "                    lambda_param=0.1,\n",
    "                    bias_adj=True,\n",
    "                    sd_cyano=sd_cyano\n",
    "                )\n",
    "\n",
    "                cont_dict[run_no] = transformed_df\n",
    "\n",
    "            else:\n",
    "                # Add to dict\n",
    "                cont_dict[run_no] = temp_df\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # Calculate correlation coefficients, convert to WFD classes and work out classification error\n",
    "\n",
    "    cc_dict = {}  # key: run_no, returns correlation coeff\n",
    "    rmse_dict = {}  # key: run_no, returns mse (pred - obs)\n",
    "    classified_dict = (\n",
    "        {}\n",
    "    )  # Key: run number. Returns df with cols 'obs' and 'pred', discrete data\n",
    "    class_error_dict = {}\n",
    "    mcc_dict = {}\n",
    "    roc_auc_dict = {}\n",
    "\n",
    "    for run_no in cont_dict.keys():\n",
    "\n",
    "        cont_df = cont_dict[run_no]  # df with continuous data\n",
    "\n",
    "        # Correlation coefficients\n",
    "        cc_dict[run_no] = cont_df[\"obs\"].corr(cont_df[\"pred\"], method=\"pearson\")\n",
    "\n",
    "        # Root mean square error\n",
    "        rmse = np.sqrt(np.mean(((cont_df[\"pred\"] - cont_df[\"obs\"]) ** 2)))\n",
    "        rmse_dict[run_no] = rmse\n",
    "\n",
    "        # Classify obs and pred into WFD (or related) class boundaries\n",
    "        disc_df = pd.DataFrame(\n",
    "            index=cont_df.index, columns=cont_df.columns\n",
    "        )  # New empty df to be populated\n",
    "        for col in cont_df.columns:\n",
    "            disc_df[col] = cont_df[col].apply(\n",
    "                lambda x: bn.discretize(bound_dict[var], x)\n",
    "            )\n",
    "        classified_dict[run_no] = disc_df\n",
    "\n",
    "        # Calculate classification error (proportion of time model predicted class correctly)\n",
    "        error = bn.classification_error(disc_df[\"obs\"], disc_df[\"pred\"])\n",
    "        class_error_dict[run_no] = error\n",
    "\n",
    "        # Calculate matthew's correlation coefficient and ROC_AUC score\n",
    "        mcc_dict[run_no] = matthews_corrcoef(\n",
    "            disc_df[\"obs\"].values, disc_df[\"pred\"].values\n",
    "        )\n",
    "        roc_auc_dict[run_no] = roc_auc_score(\n",
    "            disc_df[\"obs\"].values, disc_df[\"pred\"].values\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # Aggregate results over runs\n",
    "\n",
    "    corr_coeffs = pd.Series(cc_dict)  # These match those calculated by bnlearn\n",
    "    rmses = pd.Series(rmse_dict)\n",
    "    errors = pd.Series(class_error_dict)\n",
    "    mccs = pd.Series(mcc_dict)\n",
    "    roc_aucs = pd.Series(roc_auc_dict)\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # Take a look at the output\n",
    "    #     print(\"Correlation coefficients, %s:\" % var)\n",
    "    #     print(corr_coeffs)\n",
    "    #     print(\"\\nMean correlation coefficient, %s: %s\" % (var, corr_coeffs.mean()))\n",
    "\n",
    "    #     print(\"\\nmse, %s:\" % var)\n",
    "    #     print(mses)\n",
    "    #     print(\"Mean mse, %s: %s\" % (var, mses.mean()))\n",
    "\n",
    "    #     print(\"\\nClassification errors, %s:\" % var)\n",
    "    #     print(errors)\n",
    "    #     print(\"Mean classification error for %s: %s\" % (var, errors.mean()))\n",
    "\n",
    "    # Return dictionaries\n",
    "    #     results_dict = {\n",
    "    #                     \"corr_coeffs\": corr_coeffs,\n",
    "    #                     \"classification_errors\": errors,\n",
    "    #                     \"cont_data_dict\": cont_dict,\n",
    "    #                     \"classified_data_dict\": classified_dict,\n",
    "    #                     }\n",
    "    #     return results_dict\n",
    "    results_series = pd.Series(\n",
    "        data=np.array(\n",
    "            [\n",
    "                corr_coeffs.mean(),\n",
    "                rmses.mean(),\n",
    "                errors.mean(),\n",
    "                mccs.mean(),\n",
    "                roc_aucs.mean(),\n",
    "            ]\n",
    "        ),\n",
    "        index=[\"mean_CC\", \"mean_rmse\", \"mean_class_error\", \"mean_mcc\", \"mean_ROC_AUC\"],\n",
    "    )\n",
    "\n",
    "    return results_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chla': ['chla_cont_LOOCV_fromPredictableNodes_metno.csv',\n",
       "  'chla_cont_LOOCV_fromPredictableNodes_nomet.csv'],\n",
       " 'cyano': ['cyano_cont_LOOCV_fromPredictableNodes_nomet.csv',\n",
       "  'cyano_cont_LOOCV_fromPredictableNodes_metno.csv'],\n",
       " 'colour': ['colour_cont_LOOCV_fromPredictableNodes_nomet.csv',\n",
       "  'colour_cont_LOOCV_fromPredictableNodes_metno.csv'],\n",
       " 'TP': ['TP_cont_LOOCV_fromPredictableNodes_nomet.csv',\n",
       "  'TP_cont_LOOCV_fromPredictableNodes_metno.csv']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loop through CV results files and add to dict\n",
    "fpaths = os.listdir(CV_obs_sim_folder)\n",
    "\n",
    "fpath_dict = {}\n",
    "for file in fpaths:\n",
    "    var = file.split('_')[0]\n",
    "    if var in var_li:\n",
    "        if var in fpath_dict.keys():\n",
    "            fpath_dict[var].append(file)\n",
    "        else:\n",
    "            fpath_dict[var] = [file]\n",
    "\n",
    "fpath_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>met_data</th>\n",
       "      <th>mean_CC</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>mean_class_error</th>\n",
       "      <th>mean_mcc</th>\n",
       "      <th>mean_ROC_AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TP</td>\n",
       "      <td>metno</td>\n",
       "      <td>0.574923</td>\n",
       "      <td>3.958973</td>\n",
       "      <td>0.330263</td>\n",
       "      <td>0.335971</td>\n",
       "      <td>0.666250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TP</td>\n",
       "      <td>nomet</td>\n",
       "      <td>0.574923</td>\n",
       "      <td>3.958973</td>\n",
       "      <td>0.330263</td>\n",
       "      <td>0.335971</td>\n",
       "      <td>0.666250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chla</td>\n",
       "      <td>metno</td>\n",
       "      <td>0.546171</td>\n",
       "      <td>4.764127</td>\n",
       "      <td>0.338158</td>\n",
       "      <td>0.049825</td>\n",
       "      <td>0.519613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chla</td>\n",
       "      <td>nomet</td>\n",
       "      <td>0.541435</td>\n",
       "      <td>4.758443</td>\n",
       "      <td>0.318421</td>\n",
       "      <td>0.082392</td>\n",
       "      <td>0.530808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>colour</td>\n",
       "      <td>metno</td>\n",
       "      <td>0.846322</td>\n",
       "      <td>8.780196</td>\n",
       "      <td>0.244737</td>\n",
       "      <td>0.436296</td>\n",
       "      <td>0.706000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>colour</td>\n",
       "      <td>nomet</td>\n",
       "      <td>0.823366</td>\n",
       "      <td>9.352747</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.464775</td>\n",
       "      <td>0.727692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cyano</td>\n",
       "      <td>metno</td>\n",
       "      <td>0.374754</td>\n",
       "      <td>1.905897</td>\n",
       "      <td>0.313043</td>\n",
       "      <td>0.491434</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cyano</td>\n",
       "      <td>nomet</td>\n",
       "      <td>0.473509</td>\n",
       "      <td>1.755389</td>\n",
       "      <td>0.310870</td>\n",
       "      <td>0.494643</td>\n",
       "      <td>0.702083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Variable met_data   mean_CC  mean_rmse  mean_class_error  mean_mcc  \\\n",
       "0       TP    metno  0.574923   3.958973          0.330263  0.335971   \n",
       "1       TP    nomet  0.574923   3.958973          0.330263  0.335971   \n",
       "2     chla    metno  0.546171   4.764127          0.338158  0.049825   \n",
       "3     chla    nomet  0.541435   4.758443          0.318421  0.082392   \n",
       "4   colour    metno  0.846322   8.780196          0.244737  0.436296   \n",
       "5   colour    nomet  0.823366   9.352747          0.236842  0.464775   \n",
       "6    cyano    metno  0.374754   1.905897          0.313043  0.491434   \n",
       "7    cyano    nomet  0.473509   1.755389          0.310870  0.494643   \n",
       "\n",
       "   mean_ROC_AUC  \n",
       "0      0.666250  \n",
       "1      0.666250  \n",
       "2      0.519613  \n",
       "3      0.530808  \n",
       "4      0.706000  \n",
       "5      0.727692  \n",
       "6      0.700000  \n",
       "7      0.702083  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd_cyano = 1.29  # 1981-2018, box cox transformed with lambda\n",
    "\n",
    "series_li = []\n",
    "for var in var_li:\n",
    "    for file in fpath_dict[var]:\n",
    "        in_fpath = os.path.join(CV_obs_sim_folder, file)\n",
    "\n",
    "        # Calculate stats\n",
    "        stats_series = xval_postprocess(var, in_fpath, sd_cyano=sd_cyano)\n",
    "\n",
    "        # Tidy\n",
    "        stats_series.name = file.split('.')[0]\n",
    "        series_li.append(stats_series)\n",
    "\n",
    "df = pd.concat(series_li, axis=1, keys=[s.name for s in series_li]).transpose()\n",
    "\n",
    "df['Variable'] = [i.split('_')[0] for i in list(df.index)]\n",
    "df['met_data'] = [i.split('_')[-1] for i in list(df.index)]\n",
    "df = df.set_index(['Variable', 'met_data']).sort_index()\n",
    "\n",
    "# df['met_data'] = [i.split('_')[-1] for i in list(df.index)] # Comment out if just looking at era5 data\n",
    "# df['nodes_used'] = [i.split('_')[2][4:-5] for i in list(df.index)] # Comment out if just using predictable nodes\n",
    "# df['wind_yn'] = [i.split('_')[-2] for i in list(df.index)]\n",
    "# df.loc[df['wind_yn']!='noWind','wind_yn'] = 'Wind'\n",
    "# df = df.set_index(['Variable','met_data','nodes_used','wind_yn']).sort_index()\n",
    "\n",
    "df = df.reset_index()\n",
    "\n",
    "# Write to csv\n",
    "df.to_csv('../Data/CrossValidation/Stats/LOOCV_results_bias-adj-cyano.csv', index=False)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TP: Comparison of classification errors between continuous and discrete networks:\n",
    "\n",
    "Including met.no wind:\n",
    "- Predictable/measurable nodes, discrete BN: 0.55\n",
    "- Predictable/measurable nodes, continuous BN: 0.36\n",
    "- All nodes, discrete BN: 0.59\n",
    "- All nodes, continuous BN: 0.32\n",
    "\n",
    "**Other structures**\n",
    "\n",
    "Remove met.no wind:\n",
    "- All stats same or slightly improved without wind in continuous network. Therefore **decide to remove wind-TP link**.\n",
    "- **Comparison of discrete and continuous network classification error, without metno wind**, excluding 2019 data, becomes:\n",
    "\n",
    "    - Predictable/measurable nodes, discrete: 0.51\n",
    "    - Predictable/measurable nodes, continuous: 0.33\n",
    "    - All nodes, discrete: 0.53\n",
    "    - All nodes, continuous: 0.31\n",
    "    \n",
    "So take this as the final comparison of discrete vs continuous for TP, and part of the justification for going continuous. Other justification is the cpts in the discrete, which don't show the right behaviour (see markdown comments in BN development notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chl-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>met_data</th>\n",
       "      <th>mean_CC</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>mean_class_error</th>\n",
       "      <th>mean_mcc</th>\n",
       "      <th>mean_ROC_AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chla</td>\n",
       "      <td>era5</td>\n",
       "      <td>0.569360</td>\n",
       "      <td>4.577133</td>\n",
       "      <td>0.160256</td>\n",
       "      <td>0.674951</td>\n",
       "      <td>0.837901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chla</td>\n",
       "      <td>metno</td>\n",
       "      <td>0.546171</td>\n",
       "      <td>4.764127</td>\n",
       "      <td>0.159211</td>\n",
       "      <td>0.685710</td>\n",
       "      <td>0.844748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chla</td>\n",
       "      <td>nomet</td>\n",
       "      <td>0.541435</td>\n",
       "      <td>4.758443</td>\n",
       "      <td>0.163158</td>\n",
       "      <td>0.678740</td>\n",
       "      <td>0.841176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Variable met_data   mean_CC  mean_rmse  mean_class_error  mean_mcc  \\\n",
       "3     chla     era5  0.569360   4.577133          0.160256  0.674951   \n",
       "4     chla    metno  0.546171   4.764127          0.159211  0.685710   \n",
       "5     chla    nomet  0.541435   4.758443          0.163158  0.678740   \n",
       "\n",
       "   mean_ROC_AUC  \n",
       "3      0.837901  \n",
       "4      0.844748  \n",
       "5      0.841176  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['Variable']=='chla']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of classification error between continuous and discrete, met.no data (data to 2018):**\n",
    "\n",
    "Whole network used to predict chl-a node:\n",
    "- discrete BN:0.08\n",
    "- continuous BN: 0.21\n",
    "- continuous, adding extra WFD class (G-M boundary at 10.0): 0.44\n",
    "\n",
    "Only include nodes that will be updated when forecasting:\n",
    "- discrete BN: 0.08\n",
    "- continuous BN: 0.32\n",
    "- continuous, adding extra WFD class (G-M boundary at 10.0): 0.49\n",
    "\n",
    "Lower classification error when using the discrete network. But problems with the discrete chla cpts outweigh these differences:\n",
    "- When chla_prevSummer is high, changing wind speed from L to H doesn't result in any change to probs for low TP. But if TP is high, higher wind speed makes it more likely to have high chla. This isn't right, and is only because of a lack of data points. Therefore would have had to remove the wind-chla link.\n",
    "- Even after doing this, still have probs: when previous summer's chl-a is low, chla is always predicted to be low, and TP has no effect. When previous summer's chl-a is high, the TP effect is the wrong way around: when TP is low, expect high chla. When TP is high, have a lower chance of high TP than when TP is low. Again, just due to low data volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing wind:**\n",
    "\n",
    "- No change or slight deterioration, depending on whether you predict using just predictable nodes (no change) or the whole network (slight deterioration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>met_data</th>\n",
       "      <th>mean_CC</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>mean_class_error</th>\n",
       "      <th>mean_mcc</th>\n",
       "      <th>mean_ROC_AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cyano</td>\n",
       "      <td>era5</td>\n",
       "      <td>0.624764</td>\n",
       "      <td>0.993168</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.641449</td>\n",
       "      <td>0.820979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cyano</td>\n",
       "      <td>metno</td>\n",
       "      <td>0.632497</td>\n",
       "      <td>1.002512</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.733428</td>\n",
       "      <td>0.854167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cyano</td>\n",
       "      <td>nomet</td>\n",
       "      <td>0.679579</td>\n",
       "      <td>0.957809</td>\n",
       "      <td>0.139130</td>\n",
       "      <td>0.753961</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Variable met_data   mean_CC  mean_rmse  mean_class_error  mean_mcc  \\\n",
       "9     cyano     era5  0.624764   0.993168          0.183333  0.641449   \n",
       "10    cyano    metno  0.632497   1.002512          0.152174  0.733428   \n",
       "11    cyano    nomet  0.679579   0.957809          0.139130  0.753961   \n",
       "\n",
       "    mean_ROC_AUC  \n",
       "9       0.820979  \n",
       "10      0.854167  \n",
       "11      0.866667  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['Variable']=='cyano']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of classification error, continuous vs discrete networks (met.no data, data to 2018):**\n",
    "\n",
    "- Predictable/measurable nodes, discrete: 0.23\n",
    "- Predictable/measurable nodes, continuous: 0.17\n",
    "- All nodes, discrete: 0.13\n",
    "- All nodes, continuous: 0.18\n",
    "\n",
    "In this case, the continuous network has lower classification error than the discrete network when using predictable/measurable nodes, but slightly higher error when using all nodes (first time this has been the case). But errors still pretty low, and added benefit of the relationships being sensible between nodes rather than the cpt issue we have for one of the cpts in the cyano class: in the high summer colour class, as chla increases from L to H, the chance of high cyano decreases. I think we would expect positive relationship between chla and cyano regardless of colour, just a lower chance of high cyano when colour is higher. So this is another low data vol artefact.\n",
    "\n",
    "**To investigate in the future:**\n",
    "- Cross val of cyano predictive ability with/without colour-cyano link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed changes to structure based on cross validation results\n",
    "\n",
    "(in this notebook, and the BN_Development_1Season one)\n",
    "\n",
    "- Remove wind-TP link. Keep wind-chla link.\n",
    "- Keep rain-colour link"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
